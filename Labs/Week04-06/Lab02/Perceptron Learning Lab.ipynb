{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Classification using Perceptron Learning\n",
    "This notebook serves as the starter code and lab description covering **Chapter 19 - Learning from Examples (Part 2)** from the book *Artificial Intelligence: A Modern Approach.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install tqdm\n",
    "# pip install nltk\n",
    "# pip install numpy\n",
    "# pip install sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from numpy.random import uniform\n",
    "from tqdm import tqdm # you may comment this line if you don't need it\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "In the lecture, we discussed *linear classifiers with a hard threshold* and looked at perceptron learning update rule for model parameters. In this lab, we implement a perceptron linear classifier and use it to classify a real-world classification dataset. \n",
    "\n",
    "Though, to make things easier, we first start with a mock dataset with which we develope and test our perceptron classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Implementing the Perceptron Classifier\n",
    "To get started on the task, lets first assume a very simple set of classification points over which we want to develope our perceptron classifier. Here are the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = pd.DataFrame([[2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1]], columns=['X1', 'X2', 'Y'])\n",
    "mock_data_X = mock_data[['X1', 'X2']]\n",
    "mock_data_y = mock_data[['Y']]\n",
    "display(mock_data_X)\n",
    "display(mock_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the classifier, we clearly are trying to base our model on the *multivariable linear regression* idea, so our classifier would look like: $$h_w(\\textbf{x}_j) = w_0 + w_1x_{j,1}+w_2x_{j,2}$$ \n",
    "\n",
    "* Note: The actual *multivariable linear regression* formula goes all the way up to $w_nx_{j,n}$, but since in our mock_data we only have $n=2$ variables, we have simplified the euqation!\n",
    "\n",
    "Looking at our equation we understand that we need three $w$s to calculate $h_w(\\textbf{x}_j)$ for each row of our mock dataset. \n",
    "Start by storing the three required $w$s in a list and radomly set their values (the values would change while we run the perceptron algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a 'weights' list and fill it with three randomly generated values between -0.5 and 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the `weights` vector you just created is the best parameter set possible for classifying the mock dataset (it clearly is not, but the assumption helps you move forward and then you will revisit this assumption!), implement a `classify` function that receives one record of the mock dataset and the `weights` vector and return the classification result of the perceptron classifier which is: $$h_w(\\textbf{x}) = 1\\ \\text{if}\\ w_0 + w_1x_{j,1}+w_2x_{j,2} \\geq 0;\\ 0\\ \\text{o.w.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement `classify` function as instructed above and test it with the first row of mock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `classify` funtion, implement a `for` loop over the `mock_data` and classify each record. Print the classification result along with the actual expected value for each record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO classify and compare classification results of mock_data with the actual expected values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are that most or at least some of your datasert records are classified incorrectly (since this is a really small dataset, sometimes this does not happen, but if you re-run the experiment, you'll see its not always perfect). Now, what if I told you that one oracle setting for `weights` is `[-0.1, 0.20653640140000007, -0.23418117710000003]`. Try this oracle answer and see the change in your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try the oracle answer and repeat the past cell and show the classify results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding oracle parameter values using perceptron learning\n",
    "In this section, we are going to learn how to reach from a random `weights`vector to an oracle one. For this purpose, we will use *stochastic gradient descent (SGD)*. The only three hyperparameters we need for SGD are the *learning rate ($\\alpha$)*, the number of times we iterate over the data *(n_epochs)* while we update the model parameters (`weights` in our case), and the batch size *(b_size)* is the count of as many training data records before we update the model parameters (`weights` in our case).\n",
    "\n",
    "Here is what you need to do:\n",
    "* Implement a function and call it `perceptron_training` and have it to receive the training data, the learning rate and the number of epochs.\n",
    "* Inside `perceptron_training`:\n",
    "    * initialize the `weights` vector randomly as we did previously.\n",
    "    * implement a for loop to iterate over the training data `n_epochs` number of times.\n",
    "* Inside the `epochs` loop (which we created in the last line):\n",
    "    * create a `temp_weights` vector which will contain the parameter updates before they are sychronized with actual `weights` vector.\n",
    "    * initialize `temp_weights` to the values of `weights`.\n",
    "    * create a for loop that iterates over the training data.\n",
    "* Insider the loop over the training data:\n",
    "    * `classify` each training data record.\n",
    "    * calculate the classification $\\text{error}$ using $\\text{actual}-\\text{prediction}$.\n",
    "    * add $\\alpha * \\text{error} * \\text{input_feature}_i$ to each index $i$ of `temp_weights`.\n",
    "    * if training data record index is divisible by `b_size`, update `weights` vector with values of `temp_weights`.\n",
    "\n",
    "* Return the final modified `weights` from `perceptron_training`.\n",
    "\n",
    "At the end of each epoch, report the sum of squared values of calculate errors. We expect this value decreases while the training proceeds otherwise something is wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implemented `perceptron_training` algorithm with `l_rate = 0.1`, `n_epoch = 5`, and `b_size = 3` and use its reported weights to redo the last cell. Don't forget to print the `weights` your `perceptron_training` algorithm has found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_training(train_X, train_y, l_rate, n_epoch, b_size):\n",
    "    # TODO implement `perceptron_training` here\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use `perceptron_training` function to find the optimal `weights` \n",
    "# and using the weights you found re-classify the mock_data instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Data Application\n",
    "Now that we have the perceptron classifier we can use it to classify a real-world data; namely the `SMSSpamCollection` data which we have already used previously.\n",
    "\n",
    "Copy the `SMSSpamCollection.tsv` file from earlier lab and use `pandas` library to read and load it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO read and load up SMSSpamCollection in `sms_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how many words are there in the `TEXT` fields of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_data['TEXT'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing \n",
    "One important step to improve the accuracy of the model in dealing with weird and incorrect spellings of the information is to preform *text pre-processing*. Using the following pre-processing regular expressions develope a `clean_text` function that receives a SMS text message and lowercases and cleans it up (e.g. you can call `BAD_SYMBOLS_RE.sub` to replace bad symbols with an empty character!). Also, check all the words in the text message, and ignore them if they appear in `STOPWORDS` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only once and you're gonna be fine\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # TODO implement this function by doing the following:\n",
    "        # lowercase text\n",
    "        # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "        # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "        # delete stopwors from text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the `clean_text` function to the `TEXT` fields of the dataset and get the word count in it after clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_data['TEXT'] = sms_data['TEXT'].apply(clean_text)\n",
    "print(sms_data['TEXT'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the very familiar train/test separation of the data using `train_test_split` using 70:30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create X_train, X_test, y_train, y_test as you did in previous labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other important steps to prepare the data to be fed to the perceptron classifier we just implemented.\n",
    "\n",
    "### Vectorization\n",
    "You remember that our mock_data contained numeric values which could be fit into perceptron classification equation $$h_w(\\textbf{x}_j) = w_0 + w_1x_{j,1}+...+w_nx_{j,n}$$\n",
    "\n",
    "However, our current dataset contains strings (words) and it really doesn't make sense to multiply weights to strings. Therefore, we need to convert the words into meaningful numeric values. One important technique is to convert the messages into bag-of-word vectors. \n",
    "\n",
    "The process is simple:\n",
    "* You first collect all the words that appear in your **training data**.\n",
    "    * if there is a word in test data that does not appear in training data you are not allowed to add it to the list. You consider one single word called *out-of-vocabulary* token to account for any possible word that appears in test data and does not appear in train data. \n",
    "* Then you assign an *id* to each word.\n",
    "* To convert an SMS message into a bag-of-words:\n",
    "    * create a vector of the size of all possible distinct words in train data.\n",
    "    * initialize the vector with zeros\n",
    "    * for each word appearing in the SMS message, find its equivalent id and set bag-of-words\\[id\\] for that word to 1.\n",
    "    \n",
    "`sklearn` library has implemented this algorithm in `CountVectorizer` class and you can use it to vectorize your dataset. Here is an example and how you can do it [Link](https://thatascience.com/learn-machine-learning/bag-of-words/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO vectorize the train and test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Transformation\n",
    "The next transformation that you need to perform on the data is to reduce the effect of common words on the result of classification. The descriptions underneath this line are from the documentations of [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) in `sklearn` library:\n",
    "\n",
    "*Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.*\n",
    "\n",
    "*The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.*\n",
    "\n",
    "I recommend you read more on this transformation as it is a really useful technique in information retrieval applications. Use `TfidfTransformer` on both train and test data to prepare the data for perceptron classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Tfidf Transform the vectorized train and test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron classifier training\n",
    "Now that the training data is prepared you can train your implemented classifier to classify the test data. Give this some time as it will need time to run. You can start with `l_rate=0.1`, `n_epoch=5`, `b_size=8`, but you can do a little bit of grid search to find better models. Create `metrics.classification_report` and `metrics.confusion_matrix` results on your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO use your implemented `perceptron_training` algorithm to create proper `weights` for classification of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of your implementation with that of `sklearn`\n",
    "Now use `sklearn.linear_model.Perceptron` classifier instead of your own `perceptron_training` function and compare the performance of what you implemented and what `sklearn` has. Create the same reports as the previous cell and explain the reason of any difference in the results (if any exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "# TODO use `Perceptron` class instead of `perceptron_training`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
