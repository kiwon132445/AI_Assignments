{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - How do names look like in each language?\n",
    "This notebook serves as the starter code and lab description covering **Chapter 21 - Deep Learning (Part 2)** from the book *Artificial Intelligence: A Modern Approach.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "import string # used for defining `all_letters` variable\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "In the lecture, we have reviewed the general architecture of RNNs and we discussed their application in time varying states. In this lab, we get our hands dirty, create an RNN module and use it to train a model from the way names are represented in different languages. We then create a sampling function which uses our trained model to make new fake names for our desired language.\n",
    "\n",
    "### Data\n",
    "For this task, I have already downloaded the name data from [here](https://download.pytorch.org/tutorial/data.zip) and cleaned it up (removed accents and turned unicode to ascii) and saved it in a json file. Lets load up the data set and see a couple of names from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = json.loads(open(\"names_dataset.json\", \"r\").read())\n",
    "all_languages = list(names.keys())\n",
    "print(\"Here are all the languages for which we have some names in our dataset:\\n\\t\\t{}\\n\".format(all_languages))\n",
    "print(\"And here are some names in lets say 'Czech' language:\\n\\t\\t{}\".format(\"|\".join(names['Czech'][:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also consider the following set of all possible letters that could appear in a name (which will come in handy later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create helper functions to get random pairs of (language, name). These pairs will be used train the name generation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random language and random name from that language\n",
    "def randomTrainingPair():\n",
    "    r_lang = randomChoice(all_languages)\n",
    "    r_name = randomChoice(names[r_lang])\n",
    "    return r_lang, r_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "Now, lets turn our focus to the model design. For each timestep (that is, for each letter in a training word - e.g. name) the inputs of the network will be ``(language, current letter in name, hidden state)`` and the outputs will be ``(next letter in name, next hidden state)``. \n",
    "\n",
    "So for each training set, we'll need the language, a set of current letters, and a set of output/next\n",
    "letters converted to tensors.\n",
    "\n",
    "Fill out the next three functions which are simply receiving either a `language` or a `name` in a language and convert it to a proper format of tensor. In the process, please keep in mind that, for the RNNs, the input tensors normally are in [`one-hot`](https://en.wikipedia.org/wiki/One-hot) format while the output tensors are **not**.\n",
    "\n",
    "### `currentLetterTensor(name)`\n",
    "The first input creation method is `currentLetterTensor` which receives a string formatted `name` and creates a [`zeros`](https://pytorch.org/docs/stable/generated/torch.zeros.html) **float** tensor of size (`name_length` $\\times$ `1` $\\times$ `n_letters`). Lets call this newly created tensor `result`. Then for each letter in `name` (in position `letter_position` of the `name`), it looks up to `find` the letter index (let's call it `letter_vocab_id`) in the set of `all_letters` (vocabulary of all letters that could happen in a name). Next, it will set `result[letter_position][0][letter_vocab_id]` to `1` and returns the `result`.\n",
    "\n",
    "**Important note: don't forget to call `.to(device)` on any tensor that you create to make sure it is properly located in CPU/GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create `currentLetterTensor` function\n",
    "def currentLetterTensor(name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `languageTensor()`\n",
    "Next input creation method will be `languageTensor` (implemented pretty much like the previous one) which receives a string formatted `language` and creates a `zeros` **float** tensor of size (`1` $\\times$ `len_all_languages`). Lets call this newly created tensor `result`. Then it looks up the `index` of the passed in `language` in `all_languages` and sets `result[0][looked_up_index]` to `1` and returns the `result`. \n",
    "\n",
    "*When training we feed this tensor to the network at every timestep - this is a design choice, it could have been included as part of initial hidden state or some other strategy.*\n",
    "\n",
    "\n",
    "**Important note: don't forget to call `.to(device)` on any tensor that you create to make sure it is properly located in CPU/GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create `languageTensor` function\n",
    "def languageTensor(language):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nextLetterTensor()`\n",
    "The last method is the one that creates the output tensor which will help the loss creation module determine whether the prediction has been correct or not. Since we are predicting the next letter from the current letter for each\n",
    "timestep, the letter pairs are groups of consecutive letters from the\n",
    "line - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"),\n",
    "(\"C\", \"D\"), (\"D\", \"EOS\"). `\"EOS\"` is a dummy token that we add to the end of each name. This token teaches the network when to stop producig more letters. \n",
    "\n",
    "<img width=\"600\" src=\"https://i.imgur.com/JH58tXY.png\">\n",
    "\n",
    "Like `currentLetterTensor`, this method also receives a string formatted `name`. However, as we said, its not an input tensor so we don't need to convert the `name` into one-hot format. Instead, using the same technique that we did for `currentLetterTensor` we convert all the letters of `name` into their `letter_vocab_id` **ignoring the first letter** (since its not going to be predicted; its assumed as always to be given in our problem). We then put these converted `letter_vocab_id`s into a list (preserving the order). As the last step we append the id of **end-of-sequence** (*EOS*) token (which we have assumed to be `n_letters - 1`) to the end of the resulting list. Finally, we convert the created list of ids to a tensor by simply passing it to `torch.LongTensor` and return this converted tensor.\n",
    "\n",
    "**Important note: don't forget to call `.to(device)` on any tensor that you create to make sure it is properly located in CPU/GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create `nextLetterTensor` function\n",
    "def nextLetterTensor(name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "For convenience during training we'll make a ``getRandomTrainingExample`` function that fetches a random `(language, name)` pair and turns them into the required (`language`, `currentLetter`, `nextLetter`) tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomTrainingExample():\n",
    "    r_lang, r_name = randomTrainingPair()\n",
    "    language_tensor = languageTensor(r_lang)\n",
    "    current_letter_tensor = currentLetterTensor(r_name)\n",
    "    next_letter_tensor = nextLetterTensor(r_name)\n",
    "    return language_tensor, current_letter_tensor, next_letter_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model  Design\n",
    "Now that the training data is worked out, we need to design a proper neural network architecture based on our learning objective. This process is the same in pretty much every single new task. If you remember, earlier we mentioned that the letters are appearing one after another in the names and we want to train a network that can learn the probability of each letter given the previous letters (that have already been generated). As you know, the dependence of each letter to the letters that have appeared before itself can be modelled using a recurrent neural network.\n",
    "\n",
    "Now, we can either go ahead and use the off-the-shelf designed RNNs (e.g. [`torch.nn.GRU`](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) and [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)), or in our case, we can design a new recurrent neural network architecture. We mainly do this to learn how to make a new neural network architecture ([`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)), but it also is good because our task doesn't need lots of complexities with which the off-the-shelf designed RNNs are packaged.\n",
    "\n",
    "In this section, you will implement your first torch `Module`, but since you are new to the task, the design has already been provided to you and you will simply look at the network graph and try to implement it. Here is the designed network architecture:\n",
    "\n",
    "<img width=\"720\" src=\"Model.svg\">\n",
    "\n",
    "To implement this architecture follow this step-by-step guidelines:\n",
    "1. Create a class and name it `RNN`.\n",
    "2. Make the class inherit from `torch.nn.Module`. This way your class is automatically considered as a torch module and can be saved, loaded, and passed to loss calculators for auto-grad calculation.\n",
    "    - In addition to `__init__` function, add two other functions to your class; `forward` and `initHidden`. The following points will explain how you will implement each function.\n",
    "3. The `__init__` function is where you initialize all the components of the module. \n",
    "    - The inputs to this function would be the (`input`, `hidden layer` and `output` **sizes**)\n",
    "    - First function call would be to the `__init__` function of the supercalss (`torch.nn.Module`)\n",
    "    - Next, create the three `nn.Linear` modules as depicted in the graph design. The connections of these modules will be implemented in the `forward` function.\n",
    "    - The output size of `m1` will be of `output_size`.\n",
    "    - The output size of `m2` will be of `hidden_size`.\n",
    "    - The output size of `m3` will be of `output_size`.\n",
    "    - Also, create an instance of [`torch.nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) with `dim=-1`.\n",
    "4. The `initHidden` function will always return a `zeros` vector of size (`1` $\\times$ `hidden_size`). **Make sure you call `.to(device)` on the created tensor**.\n",
    "5. The `forward` function is the most important function which is inherited from `torch.nn.Module`. This function is automatically called when an input is passed to the module instance and it atomatically updates the gradient values in the computation graph unless `with torch.no_grad():` is in effect. In this function, you describe how your input ``(language, current letter in name, hidden state)`` is converted to the expected output  ``(next letter in name, next hidden state)``, and the only tools you have at your disposal are the modules instantiated in `__init__` function.\n",
    "    - Take `language`, `current_letter`, and `hidden_vector` as input to this function. The initial value of `hidden_vector` will be provided from `initHidden` function.\n",
    "    - As stated in the diagram, using [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html), concatenate all the input tensors and pass them to the linear modules `m1` and `m2` producing the outputs `mo1` and `mo2`. Concatenate `mo1` and `mo2` and pass it through the third linear module `m3` and pass its result through the log_softmax instance to get `res`. Return `mo1` as the next hidden state and return `res` as the produced output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement the RNN module using the step-by-step guide mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said in the previous lab, we also need a loss calculator. For training RNNs, the default training loss objective is [*Negative Log Likelihood*](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "In contrast to classification, where only the last output is used, here, we are making a prediction at every step, so we are calculating loss at every step. The magic of autograd allows you to simply sum these losses at each step and call backward at the end. The code provided in the next cell preforms all of what needs to be done to create and train our designed RNN module. If you have properly implemented the RNN module and data provider functions, this code should run smoothly. Give it a bit of time to train (I'm expecting less than 10 minutes on a GPU and between 10 to 15 minutes on a CPU). Once its done explain what it does and what is happening during training iterations.\n",
    "\n",
    "\n",
    "### TODO: explain what is happening in the next cell\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "hidden_layer_size = 128\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "\n",
    "rnn = RNN(n_letters, hidden_layer_size, n_letters)\n",
    "\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i_tqdm = tqdm(range(1, n_iters + 1))\n",
    "for iter in i_tqdm:\n",
    "    language_tensor, current_letter_tensor, next_letter_tensor = getRandomTrainingExample()\n",
    "    next_letter_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(current_letter_tensor.size(0)):\n",
    "        output, hidden = rnn(language_tensor, current_letter_tensor[i], hidden)\n",
    "        l = criterion(output, next_letter_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    output, loss = output, loss.item() / current_letter_tensor.size(0)\n",
    "    total_loss += loss\n",
    "\n",
    "    i_tqdm.set_description('Loss: %.4f' % (loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, we can also take a look at the training loss vs. training instance numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Network\n",
    "\n",
    "The last step is to perform new name generation using our trained model. To sample, we give the network a letter and ask what the next one is, feed that in as the next letter, and repeat until the *EOS token* is generated.\n",
    "\n",
    "Fill out the `sample` function using the step-by-step guidelines provided here (you can also use the code in the train cell as some help to implement this function):\n",
    "\n",
    "1.  Create tensors for input language, starting letter, and an empty hidden state.\n",
    "2.  Create a string ``output_name`` with the starting letter.\n",
    "3.  Up to the maximum expected output length:\n",
    "\n",
    "   -  Feed the `(language, current letter, hidden state)` tensors to the network.\n",
    "   -  Get the next letter from highest probability output.\n",
    "       - To find the highest probability item in output tensor, you need to call [`.topk(1)`](https://pytorch.org/docs/stable/generated/torch.topk.html) on your tensor.\n",
    "   -  If the letter is EOS, stop here.\n",
    "   -  If a regular letter, add to ``output_name`` and continue by converting the latest predicted letter to a current letter tensor.\n",
    "\n",
    "4.  Return the final name.\n",
    "\n",
    "**Note(1): since the sample method is not supposed to change the RNN network, make sure you put all the code within a `with torch.no_grad():` block to make it both faster and less memory demanding.**\n",
    "\n",
    "**Note(2): rather than having to give it a starting letter, another strategy would have been to include a \"start of string\" token in training and have the network choose its own starting letter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create `sample` function here\n",
    "def sample(language, start_letter='A', max_length = 20):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we are done implementing the `sample` function, lets generate a few names in different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(language, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(language, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('Chinese', 'CHI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
