{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Training `and` and `xor` Feedforward NNs in `torch`\n",
    "This notebook serves as the starter code and lab description covering **Chapter 21 - Deep Learning (Part 1)** from the book *Artificial Intelligence: A Modern Approach.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "# if you are using a gpu or you want your code be flexibly running over both CPU and GPU use the following line instead:\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "In the lecture, we talked about the simple feedforward networks and the way they work. In this lab, we learn to use `torch` library to create and train such networks.\n",
    "\n",
    "## First, training `and` network\n",
    "The first network that we create is a simple network to perform binary `and` operation on two binary inputs and calculate the result. For this purpose, lets first create the input/output data. In `torch`, the networks work with an object type called `Tensor`. Simply put, tensors are multi-dimensional matrices (e.g. a 1-d tensor is an actual vector, 2-d tensors are also equivalent to want you have seen as a matrix, higher than 2-d tensors are also valid and can be used to represent different dimensions of data). Read [here](https://pytorch.org/docs/stable/tensors.html) and familiarize yourself with different tensor types.\n",
    "\n",
    "In our example, we create input/output pairs as `torch.FloatTensor` as most of the loss calculation objects in torch work with floating point typed tensors. \n",
    "\n",
    "**Note(1):** you remember that in the lecture we talked about batching the input data while training. `torch` library lets you batch up many input records in one `torch.FloatTensor` object.  However, as we are not going to use multi-record batches (for the sake of simplicity), you see the records receiving a 2-dimentional list containing only one input record (e.g. torch.FloatTensor(**\\[\\[0, 0\\]\\]**)).\n",
    "\n",
    "**Note(2):** each index of input object (`data_x`) pairs up with the item with the same index in output object (`data_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are expected binary input/output pairs defined using torch tensors\n",
    "data_x = [\n",
    "    torch.FloatTensor([[0, 0]]), \n",
    "    torch.FloatTensor([[0, 1]]), \n",
    "    torch.FloatTensor([[1, 0]]), \n",
    "    torch.FloatTensor([[1, 1]])\n",
    "]\n",
    "data_y = [\n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([1])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the network input/outputs ready, we need to create the feedforward network model. Our model, however, does not have to be really complicated. It wil be just an instance of `torch.nn.Linear` (the implementation of the simple feedforward layer in `torch` library. Read more about it [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).\n",
    "\n",
    "For the next step, create an instance of `torch.nn.Linear` with the input dimension size of `2` and output dimension size of `1`. For more help, you can use the documentation of the `torch.nn.Linear` class, mentioned earlier.\n",
    "\n",
    "**Note: don't forget to call `.to(device)` on any model that you create to support underlying hardware changes from GPU to CPU and the reverse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the `model` object here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we need is an instance of a loss calculation object which can help the training process. [Here](https://pytorch.org/docs/stable/nn.html#loss-functions) you can find a complete list of all the implemented loss functions in `torch` library. For this section, we focus on mean squeared error loss function [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
    "\n",
    "Read through the documentation of `torch.nn.MSELoss` and create an instance of it with the parameter (`reduction='sum'`) which guides the network to augment the loss values for each instance by summing them up (as opposed to averaging them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create and instance of `MSELoss` and call it loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function `train` which\n",
    "1. Receives an input/output pair (e.g. one `(x,y)` pair from the pairs we stored in `data_x` and `data_y`).\n",
    " - It should also receive the model instance as well as the `learning_rate` parameter.\n",
    "2. Calculates the model prediction on `x`.\n",
    "3. Calculates the loss using the model prediction and `y`.\n",
    "4. Runs `model.zero_grad()` to reset the gradient variables of the computation graph in the network.\n",
    "5. Performs `backward()` on the loss.\n",
    "6. Updates each of model parameters (e.g. `param in model.parameters()`) with the value of `learning_rate * param.grad`.\n",
    " - Make sure this step is done under `with torch.no_grad()` block.\n",
    "7. Returns the value of loss at the end.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement `train` function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last training step would be to use the function we just implemented to train the feedforward `model` which we created earlier. \n",
    "- Set the value of `learning_rate` to `1`.\n",
    "- Iterate over the training data (pairs in `data_x` and `data_y`) for about 500 epochs (this should be done really quickly so don't worry). \n",
    "  - After each epoch if the loss of the last instance (or the average of the losses of all the instances to be more accurate!) is less than `0.01`, make sure you break the outer loop (which is performing training epochs). This is called early stopping. \n",
    "  - Print out the value of loss (you can comment this line in your submission).\n",
    "  - After each 50 epochs, half the `learning_rate` (this is called learning rate decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO perform the training loop in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will perform an evaluation on the model you just trained (I expect you get 4 `Correctly predicted` messages, if you didn't re-run the training cell and then re-run the test cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "for x, y in zip(data_x, data_y):\n",
    "    y_pred = model(x) > 0.5\n",
    "    print(x.numpy()[0], \"\\tPrediction: {}\\t{} predicted.\".format(y_pred.item(), \"Correctly\" if y.item() == y_pred.item() else \"Incorrectly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, training `xor` network\n",
    "Now that we have some hands on experience try reusing what you just implemented on the following train set representing binary `xor` operation, and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [\n",
    "    torch.FloatTensor([[0, 0]]), \n",
    "    torch.FloatTensor([[0, 1]]), \n",
    "    torch.FloatTensor([[1, 0]]), \n",
    "    torch.FloatTensor([[1, 1]])\n",
    "]\n",
    "data_y = [\n",
    "    torch.FloatTensor([1]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO re-create the model object and reuse the training loop and report the test results for it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training `xor` network with multi-layer perceptron (MLP) module\n",
    "Now lets's try a more complex model for our `xor` dataset. The model we intend to use will have two fully-connected (feedforward; `torch.nn.Linear`) layers put together using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) container. Look at [here](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to see how Sequential container works. For a better performance, have the `Sequential` instance pass the results of the first fully-connected layer through a `torch.nn.Tanh` non-linearity before passing it through the second fully-connected layer. \n",
    "\n",
    "Using this architecture, your model will first map the input of size `2` to a hidden value of size `4` (we are setting these values) and then maps the hidden vector to the output of size `1`. In the next cell, implement this model and use the `xor` data and the `train` function and the created `loss_fn` instance to train the new model.\n",
    "\n",
    "**Note(1): \\[again\\] don't forget to call `.to(device)` on any model that you create to support underlying hardware changes from GPU to CPU and the reverse.**\n",
    "\n",
    "**Note(2): since this new model has more parameters, set the initial value of `learning_rate` to `0.25` instead of `1`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the MLP module here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will perform an evaluation on the model you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[0].weight)\n",
    "print(model[0].bias)\n",
    "print(model[2].weight)\n",
    "print(model[2].bias)\n",
    "\n",
    "for x, y in zip(data_x, data_y):\n",
    "    y_pred = model(x) > 0.5\n",
    "    print(x.numpy()[0], \"\\tPrediction: {}\\t{} predicted.\".format(y_pred.item(), \"Correctly\" if y.item() == y_pred.item() else \"Incorrectly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the prediction results of the MLP model with the single layer perceptron (the one we first trained on the `xor` data) in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "You definitely need to learn `torch` in great detail. [Here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) is where you can get started. Please talk to me if you had any problem understanding the examples provided in this link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
